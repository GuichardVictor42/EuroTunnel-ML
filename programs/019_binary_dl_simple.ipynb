{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb3f025e",
   "metadata": {},
   "source": [
    "On met en place un modèle d'apprentissage supervisé pour classifier les blochets en (non)-exploitables. Une classification \"à la main\" a déjà été effectuée, et montre que la proportion de blochets non exploitables se situe autour de 18%: il s'agit donc d'un problème de classification non-équilibrée (imbalanced classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b8091f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy.random as rd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.engine.topology import Layer, InputSpec\n",
    "# from keras.optimizers import SGD\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import imblearn\n",
    "# import metrics\n",
    "import os\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f13ab8",
   "metadata": {},
   "source": [
    "On reprend la même procédure d'import des données que pour la classification à l'aide de la SOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae87564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonctions et variables utiles\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, encoding='ISO-8859-1') as f:\n",
    "        lines = f.readlines()\n",
    "        return lines\n",
    "        \n",
    "def read_number_line(some_line):\n",
    "    return [float(s) for s in some_line.split()]\n",
    "\n",
    "# my_dpi = np.sqrt(1920**2 + 1080**2)/17.3 #dpi de l'écran\n",
    "# tnorm = 60. #normalisation du temps\n",
    "# vnorm = 1.05 #normalisation de la vitesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831e199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##dossiers où récupérer les données\n",
    "\n",
    "#training\n",
    "folder = \"C:/Users/victo/stage_navier_jupyter/data_eurotunnel/Data/training/\"\n",
    "pk_folder = \"1775_20190128/\" #le PK qu'on va traiter\n",
    "\n",
    "done = [\"1775_20190128/\", \"1780_20190128/\", \"1785_20190128/\", \"1795_20190128/\", \"1800_20190128/\", \"1805_20190128/\",\n",
    "        \"1810_20190128/\", \"1815_20190128/\", \"1820_20190128/\", \"1825_20190128/\", \"1830_20190128/\", \"1835_20190128/\",\n",
    "        \"1840_20190128/\", \"3020_20190218/\", \"3025_20190218/\", \"3030_20190218/\", \"3035_20190218/\", \"3040_20190218/\",\n",
    "        \"3045_20190218/\", \"3050_20190218/\", \"3055_20190218/\", \"3060_20190218/\", \"3065_20190218/\", \"3070_20190218/\",\n",
    "        \"3075_20190218/\", \"3080_20190218/\", \"4230_20190225/\", \"4235_20190225/\", \"4240_20190225/\", \"4255_20190225/\",\n",
    "        \"4260_20190225/\"]\n",
    "failed = [\"1790_20190128/\", \"3015_20190218\"]\n",
    "\n",
    "# #validation\n",
    "# folder = \"C:/Users/victo/stage_navier_jupyter/data_eurotunnel/Data/validation/\"\n",
    "# pk_folder = \"4250_20190225/\" #le PK qu'on va traiter\n",
    "\n",
    "done_val = [\"4245_20190225/\", \"4250_20190225/\"]\n",
    "\n",
    "os.chdir(folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd5ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = folder + pk_folder #pour accéder aux fichiers .blochet du PK pk_folder\n",
    "\n",
    "mySeries = [] #series pandas\n",
    "namesofMySeries = [] #nom correspondant a chaque blochet\n",
    "\n",
    "# os.chdir(path) #current directory: C:/Users/victo/stage_navier_jupyter/data_eurotunnel/Data/training/1775-20190128/\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, encoding='ISO-8859-1') as f:\n",
    "        lines = f.readlines()\n",
    "        return lines\n",
    "    \n",
    "y_true = []\n",
    "\n",
    "for pk_folder_loop in os.listdir(folder):\n",
    "    if (pk_folder_loop + \"/\") in done:\n",
    "        path = folder + pk_folder_loop\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(\".blochet\"):\n",
    "                file_path = f\"{path}/{file}\"\n",
    "                lines = read_text_file(file_path)\n",
    "                \n",
    "                y_true.append(int(lines[-1][-1]))\n",
    "\n",
    "                times = []\n",
    "                speeds = []\n",
    "                freqs = []\n",
    "                frfs_real = []\n",
    "                frfs_imag = []\n",
    "\n",
    "                lines_to_read = lines[15:len(lines)-1] #valeurs commencent ligne 16 et saut de ligne à la fin\n",
    "                \n",
    "                for i in range(len(lines_to_read)):\n",
    "                    read_line = read_number_line(lines_to_read[i])\n",
    "                    times.append(read_line[0])\n",
    "                    speeds.append(read_line[1])\n",
    "                    freqs.append(read_line[2])\n",
    "                    frfs_real.append(read_line[3])\n",
    "                    frfs_imag.append(read_line[4])\n",
    "\n",
    "                dict = {\"Temps\":times, \"Vitesses\":speeds, \"Fréquences\":freqs, \"FRF réel\":frfs_real, \"FRF imag\":frfs_imag}\n",
    "                df = pd.DataFrame(dict)\n",
    "\n",
    "                df = df.loc[:, [\"Temps\", \"Vitesses\"]]\n",
    "                df.set_index(\"Temps\", inplace=True)\n",
    "\n",
    "                mySeries.append(df)\n",
    "                namesofMySeries.append(pk_folder_loop + \"_\" + file[:-8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b61c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_series = {namesofMySeries[i]: mySeries[i] for i in range(len(mySeries))}\n",
    "# print(dict_series.keys())\n",
    "# print(dict_series.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd8332cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4850"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mySeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a0c30ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vitesses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Temps</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.000000</th>\n",
       "      <td>0.001956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.097656</th>\n",
       "      <td>0.002443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.195312</th>\n",
       "      <td>0.001975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.292969</th>\n",
       "      <td>0.003927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.390625</th>\n",
       "      <td>0.002536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59.472660</th>\n",
       "      <td>-0.000771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59.570310</th>\n",
       "      <td>0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59.667970</th>\n",
       "      <td>0.000449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59.765630</th>\n",
       "      <td>0.000154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59.863280</th>\n",
       "      <td>-0.002553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Vitesses\n",
       "Temps              \n",
       "0.000000   0.001956\n",
       "0.097656   0.002443\n",
       "0.195312   0.001975\n",
       "0.292969   0.003927\n",
       "0.390625   0.002536\n",
       "...             ...\n",
       "59.472660 -0.000771\n",
       "59.570310  0.000244\n",
       "59.667970  0.000449\n",
       "59.765630  0.000154\n",
       "59.863280 -0.002553\n",
       "\n",
       "[614 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySeries[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfbe3915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs inscrites: {0, 1}\n",
      "Bonne longueur de liste: True\n",
      "Pourcentage de blochets inexploitables: 18.186%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true_values = {x for x in y_true}\n",
    "print(\"Valeurs inscrites: \" + str(y_true_values) + \"\\n\" + \n",
    "      \"Bonne longueur de liste: \" + str(len(mySeries)==len(y_true)) + \"\\n\" +\n",
    "      \"Pourcentage de blochets inexploitables: \" + str(round(100*(1 - np.sum(y_true)/len(y_true)), 3)) + \"%\" + \"\\n\")\n",
    "\n",
    "#valeurs en float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95bf6820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{614}\n"
     ]
    }
   ],
   "source": [
    "series_lengths = {len(series) for series in mySeries}\n",
    "print(series_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09921625",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mySeries)):\n",
    "    scaler = MinMaxScaler()\n",
    "    mySeries[i] = MinMaxScaler().fit_transform(mySeries[i])\n",
    "    mySeries[i]= mySeries[i].reshape(len(mySeries[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "898fe584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 1.0\tmin: 0.0\n",
      "[0.40812886 0.40739371 0.40760733 0.40815812 0.40736352]\n"
     ]
    }
   ],
   "source": [
    "print(\"max: \"+str(max(mySeries[0]))+\"\\tmin: \"+str(min(mySeries[0])))\n",
    "print(mySeries[0][-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe83605",
   "metadata": {},
   "source": [
    "On vectorise les données obtenues ($x$ représente les échantillons de chaque blochet et $y$ les valeurs d'exploitabilité) pour pouvoir les passer au modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2488a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4850, 614)\n",
      "(4850, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(mySeries, dtype=\"float32\")\n",
    "print(x.shape)\n",
    "y = np.array(y_true, dtype=\"uint8\")\n",
    "y = np.reshape(y, (y.shape[0], 1))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf6bb12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43668875 0.4370555  0.43670303 0.43817562 0.437126   0.4376144\n",
      " 0.43649477 0.4357442  0.43716148 0.43652102 0.43789363 0.43794847\n",
      " 0.4391713  0.43937728 0.44004813 0.43763283 0.44081575 0.44053468\n",
      " 0.43544745 0.4379443  0.43340215 0.43301603 0.42847112 0.43067583\n",
      " 0.427721   0.42901942 0.42784354 0.42715794 0.42858723 0.42816055\n",
      " 0.4274519  0.42756388 0.42805827 0.42524487 0.4276491  0.4264023\n",
      " 0.42641243 0.42678335 0.4290839  0.42943916 0.4304533  0.43138126\n",
      " 0.43217975 0.4310956  0.43193695 0.43094677 0.4307841  0.42897335\n",
      " 0.43063483 0.42795458 0.43136975 0.4291945  0.43331736 0.4311168\n",
      " 0.43790606 0.43150198 0.44112584 0.42963222 0.4436969  0.42015857\n",
      " 0.4679904  0.6194971  0.39961517 0.41887534 0.40219128 0.4148801\n",
      " 0.4107084  0.4205668  0.41834223 0.42405012 0.4219394  0.42503154\n",
      " 0.42237988 0.42453852 0.424398   0.42530525 0.42664006 0.4299704\n",
      " 0.4297875  0.43255666 0.43351457 0.43562576 0.43587273 0.43740708\n",
      " 0.43775493 0.43788072 0.43796825 0.4365975  0.43639848 0.43715042\n",
      " 0.43694124 0.43750796 0.43992096 0.44089085 0.4421962  0.442844\n",
      " 0.44314674 0.44213536 0.44100422 0.4415912  0.44046697 0.44040245\n",
      " 0.43982375 0.4394648  0.43912384 0.4403103  0.44084984 0.44162023\n",
      " 0.4427159  0.44286105 0.44227174 0.4412977  0.44004077 0.43897086\n",
      " 0.43762547 0.437942   0.4376881  0.437807   0.43946895 0.4397256\n",
      " 0.43970394 0.44037113 0.439897   0.43899253 0.4381839  0.43768904\n",
      " 0.4366892  0.43542305 0.4362819  0.434848   0.4365565  0.43521616\n",
      " 0.43850228 0.43599898 0.44042134 0.43535808 0.441218   0.4333722\n",
      " 0.44188747 0.42933732 0.44215333 0.4255103  0.44437835 0.42054835\n",
      " 0.45121694 0.41767737 0.4607726  0.40994307 0.46669933 0.45058244\n",
      " 1.         0.8601362  0.726336   0.58449143 0.01066608 0.37150198\n",
      " 0.3406491  0.5873025  0.68957806 0.28036663 0.22019756 0.\n",
      " 0.0726672  0.4452395  0.3150392  0.56731385 0.20435803 0.10185121\n",
      " 0.24849689 0.19890495 0.51408976 0.5027772  0.36752653 0.396987\n",
      " 0.14060837 0.4423372  0.44311634 0.50751793 0.5794406  0.30043963\n",
      " 0.3749973  0.37425777 0.4320959  0.6761336  0.47529018 0.51138186\n",
      " 0.3368847  0.31077123 0.5169566  0.48571992 0.6376768  0.50919837\n",
      " 0.30120397 0.37215623 0.28099695 0.52231705 0.57224125 0.5402023\n",
      " 0.5295164  0.30126342 0.38771838 0.4915181  0.57255864 0.7426777\n",
      " 0.5498561  0.5013654  0.412134   0.40391636 0.6316791  0.6575618\n",
      " 0.71851826 0.60725564 0.43495536 0.49790147 0.4687511  0.64541477\n",
      " 0.70426744 0.590162   0.5458333  0.3758529  0.4212782  0.5094485\n",
      " 0.5327629  0.60786706 0.44720417 0.36623365 0.3278027  0.31206545\n",
      " 0.47089684 0.4317457  0.41868368 0.345623   0.2205546  0.28962514\n",
      " 0.31002614 0.40627635 0.42202094 0.3070681  0.3018933  0.23308125\n",
      " 0.30137953 0.4015098  0.3962461  0.42181638 0.3166178  0.29037017\n",
      " 0.340433   0.3676362  0.4764089  0.44634208 0.40901694 0.36284477\n",
      " 0.31244424 0.4018213  0.44691253 0.47546667 0.45909268 0.36717728\n",
      " 0.37178394 0.35418713 0.41548553 0.48279226 0.4535244  0.41860026\n",
      " 0.34848294 0.32983786 0.39555496 0.4109489  0.49522123 0.42350274\n",
      " 0.3785009  0.34709558 0.36895627 0.42412156 0.48304752 0.45248955\n",
      " 0.5093781  0.32448202 0.5909895  0.2610931  0.28297266 0.52650535\n",
      " 0.4801839  0.52795815 0.44527268 0.49869767 0.5293755  0.58008105\n",
      " 0.58450335 0.5432641  0.48321155 0.49953532 0.4905731  0.5476063\n",
      " 0.5578531  0.5285991  0.49030817 0.44562054 0.46591374 0.48762056\n",
      " 0.5041991  0.5164244  0.45544255 0.4290678  0.41526115 0.42549276\n",
      " 0.4705333  0.46029112 0.44495246 0.40576446 0.3769168  0.40120158\n",
      " 0.41911954 0.45511726 0.44621953 0.40269718 0.38994938 0.37428817\n",
      " 0.40985185 0.4450432  0.45121232 0.4413567  0.39672622 0.38578412\n",
      " 0.40082788 0.42588672 0.46364358 0.44549292 0.4230157  0.38962868\n",
      " 0.3760976  0.41203538 0.42908347 0.4468973  0.43096197 0.38942364\n",
      " 0.3826938  0.37871423 0.4059778  0.4317282  0.4223338  0.40366247\n",
      " 0.37389746 0.36948523 0.38699776 0.40121216 0.42491636 0.4100306\n",
      " 0.39155743 0.38040707 0.386214   0.41626605 0.43296996 0.4361478\n",
      " 0.426036   0.40620124 0.4170254  0.4326041  0.4558913  0.46556172\n",
      " 0.45187396 0.44660103 0.44016838 0.45053315 0.473073   0.4786436\n",
      " 0.481322   0.46165127 0.44833535 0.453523   0.46480885 0.48575863\n",
      " 0.485989   0.4678236  0.45059076 0.44036558 0.45551765 0.47023797\n",
      " 0.47549385 0.47073883 0.44643193 0.43218344 0.43097994 0.4427915\n",
      " 0.45979166 0.45712155 0.4485307  0.42948708 0.41957203 0.4303109\n",
      " 0.44415903 0.45747817 0.45271623 0.43690714 0.42676815 0.42226377\n",
      " 0.43635792 0.4491071  0.4523103  0.44607347 0.4289904  0.4257093\n",
      " 0.42924473 0.43945652 0.4482561  0.44191006 0.43146282 0.4202309\n",
      " 0.41925225 0.43032795 0.4373504  0.44086415 0.43350857 0.4215496\n",
      " 0.42006135 0.42214352 0.43349984 0.4370629  0.43140337 0.42308483\n",
      " 0.4148216  0.4189228  0.42733118 0.43542764 0.43926805 0.43102923\n",
      " 0.4247869  0.4252928  0.43130893 0.44335914 0.44523075 0.44271547\n",
      " 0.4347563  0.4270036  0.43298146 0.44083005 0.4483169  0.44875142\n",
      " 0.44069457 0.4347692  0.43223736 0.43916026 0.4484344  0.45067\n",
      " 0.44720232 0.43736607 0.43234977 0.4347342  0.44091895 0.44821048\n",
      " 0.44626376 0.43890914 0.43238434 0.42913276 0.43382788 0.4410231\n",
      " 0.44298548 0.43949798 0.43070117 0.42600882 0.42777812 0.4344642\n",
      " 0.44016886 0.4380314  0.4318001  0.42565447 0.4229558  0.42935485\n",
      " 0.4332114  0.43751442 0.43279025 0.42739615 0.42229557 0.42240843\n",
      " 0.42864943 0.43201986 0.43346435 0.42913    0.42470717 0.4246657\n",
      " 0.42803478 0.4361128  0.4383811  0.43551427 0.4306316  0.428989\n",
      " 0.43259674 0.43726838 0.43842348 0.43877274 0.43668276 0.4345439\n",
      " 0.43410942 0.4381673  0.4466545  0.4458242  0.442322   0.44117284\n",
      " 0.44203678 0.44345084 0.4471056  0.45343086 0.45170626 0.44603938\n",
      " 0.44666234 0.448398   0.45091006 0.45459843 0.45752794 0.45313966\n",
      " 0.447588   0.4463209  0.44968444 0.45157817 0.45185876 0.45135283\n",
      " 0.4458196  0.44275093 0.44128942 0.44666648 0.44954574 0.44700053\n",
      " 0.44559568 0.442457   0.43682465 0.43786737 0.4426482  0.4430191\n",
      " 0.43830416 0.43610036 0.43615702 0.43353024 0.43428037 0.44124702\n",
      " 0.4412659  0.4337795  0.4330363  0.43426377 0.4330151  0.4314863\n",
      " 0.43608606 0.43400943 0.42868537 0.42409575 0.43126422 0.43123704\n",
      " 0.43198717 0.43063712 0.43176967 0.42966172 0.42569274 0.42836836\n",
      " 0.43448907 0.43156788 0.4263387  0.42572775 0.42801496 0.42863283\n",
      " 0.42525226 0.4329225  0.4337514  0.43080714 0.42518038 0.43005198\n",
      " 0.43095598 0.43576032 0.42966217 0.43061227 0.43222722 0.42921156\n",
      " 0.42614982 0.42776152 0.43326622 0.43383387 0.4311283  0.4303252\n",
      " 0.43032935 0.43404445 0.4367514  0.43391037 0.4350761  0.4333533\n",
      " 0.4347296  0.43182313 0.43478534 0.43536913 0.43702602 0.43739092\n",
      " 0.43465632 0.43508667 0.437478   0.43912292 0.4397657  0.4384152\n",
      " 0.43511802 0.43421632 0.4339841  0.43771854 0.4379208  0.4371836\n",
      " 0.43675187 0.43466416 0.435664   0.43885568 0.44066232 0.4399546\n",
      " 0.43847188 0.43616992 0.4339841  0.434631   0.4353963  0.43555158\n",
      " 0.43532857 0.43328604]\n"
     ]
    }
   ],
   "source": [
    "print(x[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4108649f",
   "metadata": {},
   "source": [
    "On sépare ensuite les 4850 données en un ensemble d'entraînement et un ensemble de test selon une proportion 80/20 à l'aide d'une fonction pré-implémentée de scikit-learn. Le paramètre stratify = y indique que l'on veut une répartition la plus homogène possible des blochets inexploitables parmi les blochets exploitables, ce qui est essentiel dans un problème de classification non équilibrée comme celui-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a41aef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_to_split, x_test, y_train_to_split, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3ab28",
   "metadata": {},
   "source": [
    "L'ensemble d'entraînement est ensuite lui-même séparé en un ensemble de training pur et un ensemble de validation, là aussi selon un principe 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3b3e496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 3104\n",
      "Number of validation samples: 776\n"
     ]
    }
   ],
   "source": [
    "num_val_samples = int(len(x_train_to_split) * 0.2)\n",
    "x_train = x_train_to_split[:-num_val_samples]\n",
    "y_train = y_train_to_split[:-num_val_samples]\n",
    "x_val = x_train_to_split[-num_val_samples:]\n",
    "y_val = y_train_to_split[-num_val_samples:]\n",
    "\n",
    "print(\"Number of training samples:\", len(x_train))\n",
    "print(\"Number of validation samples:\", len(x_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6095302",
   "metadata": {},
   "source": [
    "On analyse ensuite la répartition des classes dans l'ensemble de training pour affecter des poids différents aux deux classes (la classe \"exploitable\" codée par un \"1\" aura un poids plus faible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "addee1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples in training data: 2527 (81.41% of total)\n"
     ]
    }
   ],
   "source": [
    "counts = np.bincount(y_train[:, 0])\n",
    "print(\n",
    "    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "        counts[1], 100 * float(counts[1]) / len(y_train)\n",
    "    )\n",
    ")\n",
    "\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc01e5b",
   "metadata": {},
   "source": [
    "Les données étant déjà normalisées entre 0 et 1, on peut passer à la confection du modèle. On commence par un modèle avec des layers Dense basiques, on testera plus tard un modèle convolutionnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de0327c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                39360     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 47,745\n",
      "Trainable params: 47,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## CREATION DU MODELE\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(x_train.shape[-1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Ajout de layers convolutionnels, on part sur cette première approche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218dc85",
   "metadata": {},
   "source": [
    "On définit ensuite les métriques utilisées pour calculer l'erreur du modèle. Dans les faits, seul le taux de faux positifs ou $FPR$ défini par $\\frac{f_p}{f_p+t_n}$ nous intéresse ici. Comme celle-ci n'est pas déjà implémentée, on mesurera à la place la spécificité définie par $1 - FPR$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ef04c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metrix = [\n",
    "    metrics.FalseNegatives(name=\"fn\"),\n",
    "    metrics.FalsePositives(name=\"fp\"),\n",
    "    metrics.TrueNegatives(name=\"tn\"),\n",
    "    metrics.TruePositives(name=\"tp\"),\n",
    "#     keras.metrics.Precision(name=\"precision\"),\n",
    "#     keras.metrics.Recall(name=\"recall\"),\n",
    "#     metrics.SpecificityAtSensitivity(0.5, name=\"spec\")\n",
    "    metrics.AUC(name=\"auc\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d60b62f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.Adam(1e-2), loss=\"binary_crossentropy\", metrics=metrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e08633c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2/2 - 2s - loss: 4.8539e-04 - fn: 991.0000 - fp: 66.0000 - tn: 511.0000 - tp: 1536.0000 - auc: 0.7434 - val_loss: 1.5484 - val_fn: 547.0000 - val_fp: 2.0000 - val_tn: 127.0000 - val_tp: 100.0000 - val_auc: 0.8374\n",
      "Epoch 2/30\n",
      "2/2 - 0s - loss: 4.8962e-04 - fn: 1360.0000 - fp: 60.0000 - tn: 517.0000 - tp: 1167.0000 - auc: 0.7663 - val_loss: 0.3335 - val_fn: 11.0000 - val_fp: 104.0000 - val_tn: 25.0000 - val_tp: 636.0000 - val_auc: 0.8930\n",
      "Epoch 3/30\n",
      "2/2 - 0s - loss: 4.8528e-04 - fn: 35.0000 - fp: 385.0000 - tn: 192.0000 - tp: 2492.0000 - auc: 0.8736 - val_loss: 0.3338 - val_fn: 51.0000 - val_fp: 25.0000 - val_tn: 104.0000 - val_tp: 596.0000 - val_auc: 0.9042\n",
      "Epoch 4/30\n",
      "2/2 - 0s - loss: 2.6091e-04 - fn: 467.0000 - fp: 77.0000 - tn: 500.0000 - tp: 2060.0000 - auc: 0.8972 - val_loss: 0.8246 - val_fn: 295.0000 - val_fp: 7.0000 - val_tn: 122.0000 - val_tp: 352.0000 - val_auc: 0.8944\n",
      "Epoch 5/30\n",
      "2/2 - 0s - loss: 3.4254e-04 - fn: 1003.0000 - fp: 39.0000 - tn: 538.0000 - tp: 1524.0000 - auc: 0.8718 - val_loss: 0.3745 - val_fn: 58.0000 - val_fp: 25.0000 - val_tn: 104.0000 - val_tp: 589.0000 - val_auc: 0.9033\n",
      "Epoch 6/30\n",
      "2/2 - 0s - loss: 2.4532e-04 - fn: 183.0000 - fp: 107.0000 - tn: 470.0000 - tp: 2344.0000 - auc: 0.9067 - val_loss: 0.3102 - val_fn: 23.0000 - val_fp: 59.0000 - val_tn: 70.0000 - val_tp: 624.0000 - val_auc: 0.8981\n",
      "Epoch 7/30\n",
      "2/2 - 0s - loss: 3.0721e-04 - fn: 78.0000 - fp: 245.0000 - tn: 332.0000 - tp: 2449.0000 - auc: 0.9035 - val_loss: 0.3356 - val_fn: 33.0000 - val_fp: 31.0000 - val_tn: 98.0000 - val_tp: 614.0000 - val_auc: 0.9015\n",
      "Epoch 8/30\n",
      "2/2 - 0s - loss: 2.4524e-04 - fn: 191.0000 - fp: 109.0000 - tn: 468.0000 - tp: 2336.0000 - auc: 0.9141 - val_loss: 0.5099 - val_fn: 112.0000 - val_fp: 15.0000 - val_tn: 114.0000 - val_tp: 535.0000 - val_auc: 0.9054\n",
      "Epoch 9/30\n",
      "2/2 - 0s - loss: 2.7395e-04 - fn: 524.0000 - fp: 55.0000 - tn: 522.0000 - tp: 2003.0000 - auc: 0.9012 - val_loss: 0.5044 - val_fn: 106.0000 - val_fp: 15.0000 - val_tn: 114.0000 - val_tp: 541.0000 - val_auc: 0.9053\n",
      "Epoch 10/30\n",
      "2/2 - 0s - loss: 2.5681e-04 - fn: 411.0000 - fp: 64.0000 - tn: 513.0000 - tp: 2116.0000 - auc: 0.9104 - val_loss: 0.3544 - val_fn: 44.0000 - val_fp: 26.0000 - val_tn: 103.0000 - val_tp: 603.0000 - val_auc: 0.9028\n",
      "Epoch 11/30\n",
      "2/2 - 0s - loss: 2.4747e-04 - fn: 151.0000 - fp: 119.0000 - tn: 458.0000 - tp: 2376.0000 - auc: 0.9095 - val_loss: 0.3154 - val_fn: 28.0000 - val_fp: 36.0000 - val_tn: 93.0000 - val_tp: 619.0000 - val_auc: 0.9018\n",
      "Epoch 12/30\n",
      "2/2 - 0s - loss: 2.5680e-04 - fn: 118.0000 - fp: 144.0000 - tn: 433.0000 - tp: 2409.0000 - auc: 0.9120 - val_loss: 0.3513 - val_fn: 48.0000 - val_fp: 26.0000 - val_tn: 103.0000 - val_tp: 599.0000 - val_auc: 0.9043\n",
      "Epoch 13/30\n",
      "2/2 - 0s - loss: 2.3859e-04 - fn: 232.0000 - fp: 91.0000 - tn: 486.0000 - tp: 2295.0000 - auc: 0.9106 - val_loss: 0.4412 - val_fn: 84.0000 - val_fp: 19.0000 - val_tn: 110.0000 - val_tp: 563.0000 - val_auc: 0.9069\n",
      "Epoch 14/30\n",
      "2/2 - 0s - loss: 2.5281e-04 - fn: 396.0000 - fp: 60.0000 - tn: 517.0000 - tp: 2131.0000 - auc: 0.9031 - val_loss: 0.3857 - val_fn: 63.0000 - val_fp: 22.0000 - val_tn: 107.0000 - val_tp: 584.0000 - val_auc: 0.9076\n",
      "Epoch 15/30\n",
      "2/2 - 0s - loss: 2.3184e-04 - fn: 247.0000 - fp: 72.0000 - tn: 505.0000 - tp: 2280.0000 - auc: 0.9121 - val_loss: 0.2986 - val_fn: 39.0000 - val_fp: 29.0000 - val_tn: 100.0000 - val_tp: 608.0000 - val_auc: 0.9040\n",
      "Epoch 16/30\n",
      "2/2 - 0s - loss: 2.3275e-04 - fn: 145.0000 - fp: 109.0000 - tn: 468.0000 - tp: 2382.0000 - auc: 0.9182 - val_loss: 0.2938 - val_fn: 40.0000 - val_fp: 29.0000 - val_tn: 100.0000 - val_tp: 607.0000 - val_auc: 0.9045\n",
      "Epoch 17/30\n",
      "2/2 - 0s - loss: 2.3205e-04 - fn: 170.0000 - fp: 98.0000 - tn: 479.0000 - tp: 2357.0000 - auc: 0.9142 - val_loss: 0.3454 - val_fn: 60.0000 - val_fp: 23.0000 - val_tn: 106.0000 - val_tp: 587.0000 - val_auc: 0.9082\n",
      "Epoch 18/30\n",
      "2/2 - 0s - loss: 2.3023e-04 - fn: 279.0000 - fp: 78.0000 - tn: 499.0000 - tp: 2248.0000 - auc: 0.9121 - val_loss: 0.3653 - val_fn: 72.0000 - val_fp: 22.0000 - val_tn: 107.0000 - val_tp: 575.0000 - val_auc: 0.9074\n",
      "Epoch 19/30\n",
      "2/2 - 0s - loss: 2.2640e-04 - fn: 284.0000 - fp: 75.0000 - tn: 502.0000 - tp: 2243.0000 - auc: 0.9158 - val_loss: 0.3046 - val_fn: 50.0000 - val_fp: 26.0000 - val_tn: 103.0000 - val_tp: 597.0000 - val_auc: 0.9060\n",
      "Epoch 20/30\n",
      "2/2 - 0s - loss: 2.2702e-04 - fn: 195.0000 - fp: 89.0000 - tn: 488.0000 - tp: 2332.0000 - auc: 0.9119 - val_loss: 0.2828 - val_fn: 44.0000 - val_fp: 26.0000 - val_tn: 103.0000 - val_tp: 603.0000 - val_auc: 0.9066\n",
      "Epoch 21/30\n",
      "2/2 - 0s - loss: 2.3328e-04 - fn: 170.0000 - fp: 91.0000 - tn: 486.0000 - tp: 2357.0000 - auc: 0.9071 - val_loss: 0.3187 - val_fn: 55.0000 - val_fp: 23.0000 - val_tn: 106.0000 - val_tp: 592.0000 - val_auc: 0.9079\n",
      "Epoch 22/30\n",
      "2/2 - 0s - loss: 2.2296e-04 - fn: 233.0000 - fp: 83.0000 - tn: 494.0000 - tp: 2294.0000 - auc: 0.9124 - val_loss: 0.3438 - val_fn: 63.0000 - val_fp: 22.0000 - val_tn: 107.0000 - val_tp: 584.0000 - val_auc: 0.9082\n",
      "Epoch 23/30\n",
      "2/2 - 0s - loss: 2.2163e-04 - fn: 268.0000 - fp: 69.0000 - tn: 508.0000 - tp: 2259.0000 - auc: 0.9151 - val_loss: 0.3037 - val_fn: 50.0000 - val_fp: 26.0000 - val_tn: 103.0000 - val_tp: 597.0000 - val_auc: 0.9067\n",
      "Epoch 24/30\n",
      "2/2 - 0s - loss: 2.1934e-04 - fn: 197.0000 - fp: 87.0000 - tn: 490.0000 - tp: 2330.0000 - auc: 0.9170 - val_loss: 0.2882 - val_fn: 46.0000 - val_fp: 26.0000 - val_tn: 103.0000 - val_tp: 601.0000 - val_auc: 0.9089\n",
      "Epoch 25/30\n",
      "2/2 - 0s - loss: 2.1564e-04 - fn: 180.0000 - fp: 88.0000 - tn: 489.0000 - tp: 2347.0000 - auc: 0.9188 - val_loss: 0.3281 - val_fn: 56.0000 - val_fp: 23.0000 - val_tn: 106.0000 - val_tp: 591.0000 - val_auc: 0.9091\n",
      "Epoch 26/30\n",
      "2/2 - 0s - loss: 2.2139e-04 - fn: 239.0000 - fp: 74.0000 - tn: 503.0000 - tp: 2288.0000 - auc: 0.9120 - val_loss: 0.3396 - val_fn: 60.0000 - val_fp: 23.0000 - val_tn: 106.0000 - val_tp: 587.0000 - val_auc: 0.9089\n",
      "Epoch 27/30\n",
      "2/2 - 0s - loss: 2.2380e-04 - fn: 242.0000 - fp: 80.0000 - tn: 497.0000 - tp: 2285.0000 - auc: 0.9112 - val_loss: 0.3185 - val_fn: 52.0000 - val_fp: 25.0000 - val_tn: 104.0000 - val_tp: 595.0000 - val_auc: 0.9090\n",
      "Epoch 28/30\n",
      "2/2 - 0s - loss: 2.1444e-04 - fn: 222.0000 - fp: 79.0000 - tn: 498.0000 - tp: 2305.0000 - auc: 0.9189 - val_loss: 0.3087 - val_fn: 49.0000 - val_fp: 25.0000 - val_tn: 104.0000 - val_tp: 598.0000 - val_auc: 0.9075\n",
      "Epoch 29/30\n",
      "2/2 - 0s - loss: 2.1255e-04 - fn: 190.0000 - fp: 85.0000 - tn: 492.0000 - tp: 2337.0000 - auc: 0.9204 - val_loss: 0.3047 - val_fn: 49.0000 - val_fp: 25.0000 - val_tn: 104.0000 - val_tp: 598.0000 - val_auc: 0.9096\n",
      "Epoch 30/30\n",
      "2/2 - 0s - loss: 2.1965e-04 - fn: 196.0000 - fp: 89.0000 - tn: 488.0000 - tp: 2331.0000 - auc: 0.9143 - val_loss: 0.3329 - val_fn: 55.0000 - val_fp: 23.0000 - val_tn: 106.0000 - val_tp: 592.0000 - val_auc: 0.9099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e9000576d0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=2048,\n",
    "    epochs=30,\n",
    "    verbose=2,\n",
    "    validation_data=(x_val, y_val),\n",
    "    class_weight=class_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298d9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
